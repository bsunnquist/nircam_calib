{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSEG data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code that can be used to validate data created during GSEG3. It reads in the APT-derived xml and pointing files and constructs a dictionary of expected data properties. It then compares these properties to the information contained in the headers of the actual data to look for inconsistencies.\n",
    "\n",
    "**Mirage and pysiaf are dependencies. Be sure they are installed in your environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "from mirage.yaml import yaml_generator\n",
    "from mirage.apt import apt_inputs\n",
    "from mirage.seed_image.catalog_seed_image import Catalog_seed\n",
    "from mirage.utils.siaf_interface import sci_subarray_corners\n",
    "from mirage.utils.utils import calc_frame_time\n",
    "from mirage.yaml.generate_observationlist import get_observation_dict\n",
    "import numpy as np\n",
    "import os\n",
    "import pkg_resources\n",
    "import pysiaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observations to validate (use 3 characters); if you want to validate all\n",
    "# observations in a proposal, just set observations = []\n",
    "observations = ['125','126','127','128','129','130','131','132','133','134','135',\n",
    "                '136','137','138','139','140','141','142','143','144','145','146']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header keywords to check against APT-derived dictionary\n",
    "# It is assumed that these header keywords exist in both the uncal and rate images\n",
    "KEYWORDS = ['SUBARRAY', 'DETECTOR', 'NINTS', 'NGROUPS', 'NAXIS', 'EFFEXPTM',\n",
    "            'LONGFILTER', 'LONGPUPIL', 'SHORTFILTER', 'SHORTPUPIL', 'READPATT',\n",
    "            'OBSLABEL', 'EXP_TYPE', 'TITLE', 'OBSERVTN', 'TEMPLATE',\n",
    "            'EXPRIPAR', 'SUBSTRT1', 'SUBSTRT2', 'SUBSIZE1', 'SUBSIZE2',\n",
    "            'FASTAXIS', 'SLOWAXIS', 'PATTTYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corresponding keywords in the APT-derived dictionary\n",
    "# These must correspond one-to-one with KEYWORDS above\n",
    "TABLE_KEYWORDS = ['Subarray', None, 'Integrations', 'Groups', None, None,\n",
    "                  'LongFilter', 'LongPupil', 'ShortFilter', 'ShortPupil', 'ReadoutPattern',\n",
    "                  'ObservationName', 'Mode', 'Title', 'ObservationID', 'APTTemplate',\n",
    "                  'ParallelInstrument', None, None, None, None,\n",
    "                  None, None, 'PrimaryDitherType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of header keywords and their expected values in the \n",
    "# PRIMARY and SCI extensions of the UNCAL, DARK, and RATE files.\n",
    "# These are useful when you know exactly what these keywords should be\n",
    "# for every file (e.g. NAXIS should always be 2 in the RATE image SCI header).\n",
    "# These will be checked in addition to the KEYWORDS checks above\n",
    "UNCAL_PRIMARY_KEYWORDS = {}\n",
    "UNCAL_SCI_KEYWORDS = {'BITPIX':16, 'NAXIS':4, 'BUNIT':'DN'}\n",
    "DARK_PRIMARY_KEYWORDS = {}\n",
    "DARK_SCI_KEYWORDS = {'NAXIS':4, 'BUNIT':'DN'}\n",
    "RATE_PRIMARY_KEYWORDS = {}\n",
    "RATE_SCI_KEYWORDS = {'BITPIX':-32, 'NAXIS':2, 'BUNIT':'DN/s'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PRIMARY and SCI header keywords to store for each file in the output summary table\n",
    "SUMMARY_TABLE_PRIMARY = ['FILENAME', 'DETECTOR', 'FILTER', 'PUPIL', 'EXP_TYPE', 'READPATT', 'NINTS', 'NGROUPS',\n",
    "                         'NFRAMES', 'GROUPGAP', 'SUBARRAY', 'SUBSIZE1', 'SUBSIZE2', 'APERNAME']\n",
    "SUMMARY_TABLE_SCI = ['BITPIX', 'NAXIS', 'NAXIS1', 'NAXIS2', 'NAXIS3', 'NAXIS4', 'BUNIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEGER_KEYWORDS = ['Integrations', 'Groups']\n",
    "FLOAT_KEYWORDS = ['EFFEXPTM']\n",
    "FILTER_KEYWORDS = ['LONGFILTER', 'LONGPUPIL', 'SHORTFILTER', 'SHORTPUPIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FASTAXIS and SLOWAXIS\n",
    "HORIZONTAL_FLIP = ['NRCA1', 'NRCA3', 'NRCALONG', 'NRCB2', 'NRCB4']\n",
    "VERTICAL_FLIP = ['NRCA2', 'NRCA4', 'NRCB1', 'NRCB3', 'NRCBLONG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected detectors used for each module_subarray combo (these are not always perfect)\n",
    "DETECTOR_DICT = {'ALL_FULL':['NRCA1', 'NRCA2', 'NRCA3', 'NRCA4', 'NRCALONG',\n",
    "                             'NRCB1', 'NRCB2', 'NRCB3', 'NRCB4', 'NRCBLONG'],\n",
    "                 'A_FULL':['NRCA1', 'NRCA2', 'NRCA3', 'NRCA4', 'NRCALONG'],\n",
    "                 'A_SUBGRISM256':['NRCA1', 'NRCA3','NRCALONG'],\n",
    "                 'A_SUBGRISM128':['NRCA1', 'NRCA3','NRCALONG'],\n",
    "                 'A_SUBGRISM64':['NRCA1', 'NRCA3','NRCALONG'],\n",
    "                 'B_FULL':['NRCB1', 'NRCB2', 'NRCB3', 'NRCB4', 'NRCBLONG'],\n",
    "                 'B_SUB640':['NRCB1', 'NRCB2', 'NRCB3', 'NRCB4', 'NRCBLONG'],\n",
    "                 'B_SUB320':['NRCB1', 'NRCB2', 'NRCB3', 'NRCB4', 'NRCBLONG'],\n",
    "                 'B_SUB160':['NRCB1', 'NRCB2', 'NRCB3', 'NRCB4', 'NRCBLONG'],\n",
    "                 'B_SUB400P':['NRCB1', 'NRCBLONG'],\n",
    "                 'B_SUB160P':['NRCB1', 'NRCBLONG'],\n",
    "                 'B_SUB64P':['NRCB1', 'NRCBLONG']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(filename, summary_dict):\n",
    "    \"\"\"Adds an entry for filename to the output summary dict\n",
    "    \"\"\"\n",
    "    for key in SUMMARY_TABLE_PRIMARY:\n",
    "        try:\n",
    "            val = str(fits.getheader(filename, 'PRIMARY')[key])\n",
    "        except KeyError:\n",
    "            val = ''\n",
    "        summary_dict[key].append(val)\n",
    "    \n",
    "    for key in SUMMARY_TABLE_SCI:\n",
    "        try:\n",
    "            val = str(fits.getheader(filename, 'SCI')[key])\n",
    "        except KeyError:\n",
    "            val = ''\n",
    "        summary_dict[key].append(val)\n",
    "    \n",
    "    return summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_exptype(value):\n",
    "    \"\"\"Modify the exposure type as listed in the exposure table\n",
    "    to match one of the strings as used in the fits files.\n",
    "    e.g. 'imaging' becomes 'NRC_IMAGE'\n",
    "    Remember that currently, Mirage only knows imaging and wfss\n",
    "    \"\"\"\n",
    "    if value == 'imaging':\n",
    "        return 'NRC_IMAGE'\n",
    "    elif value == 'wfss':\n",
    "        return 'NRC_GRISM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_files(exp_dict, index):\n",
    "    \"\"\"Calculate the total number of files expected for an\n",
    "    observation based on the number of dithers and the \n",
    "    module/subarray combination.\n",
    "    \"\"\"\n",
    "    module = exp_dict['Module'][index].upper()\n",
    "    subarray = exp_dict['Subarray'][index].upper()\n",
    "    subarray = subarray.replace('DHSPILA', '').replace('DHSPILB', '')\n",
    "    number_of_dithers = exp_dict['number_of_dithers'][index]\n",
    "    combo = '{}_{}'.format(module, subarray)\n",
    "    \n",
    "    # Find expected number of detectors based on module/subarray combo\n",
    "    try:\n",
    "        expected_detectors = DETECTOR_DICT[combo]\n",
    "        n_detectors = len(expected_detectors)\n",
    "    except KeyError:\n",
    "        print('Warning: module/subbary={} not an expected '\n",
    "              'combination; assuming all 10 detectors used.'.format(combo))\n",
    "        n_detectors = 10\n",
    "        expected_detectors = DETECTOR_DICT['ALL_FULL']\n",
    "    \n",
    "    # Find total expected number of files/detectors based on detectors/dithers\n",
    "    total = number_of_dithers * n_detectors\n",
    "    expected_detectors = expected_detectors * number_of_dithers\n",
    "    \n",
    "    return total, expected_detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_apername(sub, aperture):\n",
    "    \"\"\"Check that APT subarray is consistent with header APERNAME\"\"\"\n",
    "    n = [s for s in sub if s.isdigit()]\n",
    "    n = [''.join(n[:])][0]  # number from subarray\n",
    "    nn = [s for s in aperture.split('_')[1] if s.isdigit()]  # split[1] to get rid of ends of e.g. NRCA5_GRISM256_F277W\n",
    "    nn = [''.join(nn[:])][0]  # number from header aperture\n",
    "    if ((n != nn) |\n",
    "        (('GRISM' in sub) & ('GRISM' not in aperture)) |\n",
    "        (('GRISM' in aperture) & ('GRISM' not in sub)) |\n",
    "        ((n+'P' in sub) & (n+'P' not in aperture)) |\n",
    "        ((n+'P' in aperture) & (n+'P' not in sub)) |\n",
    "        (('FULL' in sub) & ('FULL' not in aperture)) |\n",
    "        (('FULL' in aperture) & ('FULL' not in sub))):\n",
    "        print('WARNING: Mismatch between APT Subarray ({}) and APERNAME ({})'.format(sub, aperture)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_detector_exposures(matching_uncal_files, matching_rate_files, expected_detectors):\n",
    "    \"\"\"Ensures both an uncal and rate file exists for each expected detector.\n",
    "    \"\"\"\n",
    "    expected_detectors = np.array(expected_detectors)\n",
    "    for detector in expected_detectors:\n",
    "        n_dets = len(expected_detectors[expected_detectors==detector])\n",
    "        n_files = 0\n",
    "        for f in matching_uncal_files:\n",
    "            if detector.lower() in f:\n",
    "                n_files += 1\n",
    "        if n_files != n_dets:\n",
    "            print('WARNING: Expected {} uncal files for detector {}, but found {}'.format(n_dets, detector, n_files))\n",
    "        n_files = 0\n",
    "        for f in matching_rate_files:\n",
    "            if detector.lower() in f:\n",
    "                n_files += 1\n",
    "        if n_files != n_dets:\n",
    "            print('WARNING: Expected {} rate files for detector {}, but found {}'.format(n_dets, detector, n_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_file_lists(uncal, rate):\n",
    "    \"\"\"Given lists of uncal and rate files corresponding to a single\n",
    "    observation, adjust the lists to be the same length, adding in\n",
    "    None for any files that are missing in a given list\n",
    "    \"\"\"\n",
    "    udict = {}\n",
    "    rdict = {}\n",
    "    expanded_rate = []\n",
    "    expanded_uncal = []\n",
    "\n",
    "    # Loop through uncal files and look for matching rate files\n",
    "    for ufile in uncal:\n",
    "        dirname, filename = os.path.split(ufile)\n",
    "        base = filename.strip('_uncal.fits')\n",
    "        fullbase = os.path.join(dirname, base)\n",
    "        found = False\n",
    "        for rfile in rate:\n",
    "            if fullbase in rfile:\n",
    "                found = True\n",
    "                break\n",
    "        udict[fullbase] = found\n",
    "\n",
    "    # Loop through rate files and look for matching uncal files\n",
    "    for rfile in rate:\n",
    "        dirname, filename = os.path.split(rfile)\n",
    "        base = filename.strip('_rate.fits')\n",
    "        fullbase = os.path.join(dirname, base)\n",
    "        found = False\n",
    "        for ufile in uncal:\n",
    "            if fullbase in ufile:\n",
    "                found = True\n",
    "                break\n",
    "        rdict[fullbase] = found\n",
    "\n",
    "    # Fill in missing files, in either uncal or rate lists,\n",
    "    # with None\n",
    "    for ukey in udict:\n",
    "        expanded_uncal.append(ukey + '_uncal.fits')\n",
    "        if udict[ukey]:\n",
    "            expanded_rate.append(ukey + '_rate.fits')\n",
    "        else:\n",
    "            expanded_rate.append(None)\n",
    "    for rkey in rdict:\n",
    "        if not rdict[rkey]:\n",
    "            expanded_rate.append(rkey + '_rate.fits')\n",
    "            expanded_uncal.append(None)\n",
    "    return expanded_uncal, expanded_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fastaxis(detector):\n",
    "    \"\"\"Identify the values of FASTAXIS and SLOWAXIS based on the detector\n",
    "    name\n",
    "    \"\"\"\n",
    "    if detector in HORIZONTAL_FLIP:\n",
    "        fast = -1\n",
    "        slow = 2\n",
    "    elif detector in VERTICAL_FLIP:\n",
    "        fast = 1\n",
    "        slow = -2\n",
    "    return fast, slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rate_files(gseg_uncal_files):\n",
    "    \"\"\"Returns a list of rate files that correspond to the \n",
    "    input uncal files. These may be either rate.fits files \n",
    "    or dark.fits files.\"\"\"\n",
    "    \n",
    "    gseg_rate_files = []\n",
    "    for f in gseg_uncal_files:\n",
    "        rate_file = f.replace('uncal.fits','rate.fits')\n",
    "        dark_file = f.replace('uncal.fits','dark.fits')\n",
    "        if os.path.isfile(rate_file):\n",
    "            gseg_rate_files.append(rate_file)\n",
    "        elif os.path.isfile(dark_file):\n",
    "            gseg_rate_files.append(dark_file)\n",
    "        else:\n",
    "            print('Warning: No corresponding rate file for {}'.format(f))\n",
    "    \n",
    "    return gseg_rate_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    \"\"\"Read in the given fits file and return the data and header\n",
    "    \"\"\"\n",
    "    with fits.open(filename) as h:\n",
    "        signals = h['SCI'].data\n",
    "        header0 = h[0].header\n",
    "        header1 = h[1].header\n",
    "    return signals, header0, header1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_shape(sub):\n",
    "    \"\"\"Returns the expected shape of the science data\n",
    "    based on the input APT subarray.\n",
    "    \"\"\"\n",
    "    siaf = pysiaf.Siaf('NIRCam')\n",
    "    subarray = sub.replace('SUB', '').replace('DHSPILA', '').replace('DHSPILB', '')\n",
    "    \n",
    "    if 'FULL' in subarray:\n",
    "        expected_shape = (2048, 2048)\n",
    "    else:\n",
    "        # needed to be careful here to remove cases where e.g. SUB64 was in SUB640\n",
    "        similar_aps = [aper for aper in siaf.apernames if subarray in aper and subarray+'0' not in aper]\n",
    "        if len(similar_aps) == 0:\n",
    "            print('WARNING: Cannot find expected shape for subarray {}'.format(sub))\n",
    "            expected_shape = (-99, -99)\n",
    "        else:\n",
    "            # just use first entry to get expected shape since they should all be the same\n",
    "            similar_ap = similar_aps[0]\n",
    "            expected_shape = (siaf[similar_ap].YSciSize, siaf[similar_ap].XSciSize)\n",
    "    \n",
    "    return expected_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_keywords(head):\n",
    "    \"\"\"Extract values for the desired keywords from the given header\n",
    "    \"\"\"\n",
    "    file_info = {}\n",
    "    for keyword in KEYWORDS:\n",
    "        try:\n",
    "            info = head[keyword]\n",
    "        except KeyError:\n",
    "            if 'FILTER' in keyword:\n",
    "                info = head['FILTER']\n",
    "            elif 'PUPIL' in keyword:\n",
    "                info = head['PUPIL']\n",
    "            else:\n",
    "                info = None\n",
    "\n",
    "        file_info[keyword] = info\n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_info(values, index):\n",
    "    \"\"\"Extract information from the exposure table that matches the\n",
    "    header keyword values in KEYWORDS\n",
    "    \"\"\"\n",
    "    values_dict = {}\n",
    "    for table_keyword, file_keyword in zip(TABLE_KEYWORDS, KEYWORDS):\n",
    "        if table_keyword is not None:\n",
    "            if table_keyword in INTEGER_KEYWORDS:\n",
    "                value = int(values[table_keyword][index])\n",
    "            else:\n",
    "                value = values[table_keyword][index]\n",
    "            values_dict[file_keyword] = value\n",
    "        else:\n",
    "            values_dict[file_keyword] = None\n",
    "    return values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dimensions(filename, file_type, expected_shape):\n",
    "    \"\"\"Verify the header and data dimensions for each extension.\n",
    "    \"\"\"\n",
    "    header = fits.getheader(filename, 'PRIMARY')\n",
    "    if file_type == 'UNCAL':\n",
    "        extensions = ['SCI']\n",
    "        primary_header_shape = (header['NINTS'], header['NGROUPS'], header['SUBSIZE2'], header['SUBSIZE1'])\n",
    "        for ext in extensions:\n",
    "            try:\n",
    "                header = fits.getheader(filename, ext)\n",
    "                data_shape = fits.getdata(filename, ext).shape\n",
    "                naxis_shape = (header['NAXIS4'], header['NAXIS3'], header['NAXIS2'], header['NAXIS1'])\n",
    "                if ((primary_header_shape != data_shape) | (primary_header_shape != naxis_shape) | \n",
    "                    (primary_header_shape[-2:] != expected_shape)):\n",
    "                    print('WARNING: Data dimensions incorrect')\n",
    "                    print('Expected image shape: {}'.format(expected_shape))\n",
    "                    print('PRIMARY header shape: {}'.format(primary_header_shape))\n",
    "                    print('{} header shape: {}'.format(ext, naxis_shape))\n",
    "                    print('{} data shape: {}'.format(ext, data_shape)) \n",
    "            except KeyError:\n",
    "                print('Cannot verify shape for {} extension'.format(ext))\n",
    "    elif file_type == 'DARK':\n",
    "        extensions = 'SCI PIXELDQ GROUPDQ ERR'.split()\n",
    "        primary_header_shape = (header['NINTS'], header['NGROUPS'], header['SUBSIZE2'], header['SUBSIZE1'])\n",
    "        for ext in extensions:\n",
    "            try:\n",
    "                header = fits.getheader(filename, ext)\n",
    "                data_shape = fits.getdata(filename, ext).shape\n",
    "                # PIXELDQ extension is only 2D and doesnt have NAXIS3/4, \n",
    "                # so just make it 4D by appending expected results\n",
    "                if ext == 'PIXELDQ':\n",
    "                    data_shape = (primary_header_shape[0], primary_header_shape[1], data_shape[0], data_shape[1])\n",
    "                    naxis_shape = (primary_header_shape[0], primary_header_shape[1], header['NAXIS2'], header['NAXIS1'])\n",
    "                else:\n",
    "                    naxis_shape = (header['NAXIS4'], header['NAXIS3'], header['NAXIS2'], header['NAXIS1'])\n",
    "                if ((primary_header_shape != data_shape) | (primary_header_shape != naxis_shape) | \n",
    "                    (primary_header_shape[-2:] != expected_shape)):\n",
    "                    print('WARNING: Data dimensions incorrect')\n",
    "                    print('Expected image shape: {}'.format(expected_shape))\n",
    "                    print('PRIMARY header shape: {}'.format(primary_header_shape))\n",
    "                    print('{} header shape: {}'.format(ext, naxis_shape))\n",
    "                    print('{} data shape: {}'.format(ext, data_shape))\n",
    "            except KeyError:\n",
    "                print('Cannot verify shape for {} extension'.format(ext))\n",
    "    elif file_type == 'RATE':\n",
    "        extensions = 'SCI ERR DQ VAR_POISSON VAR_RNOISE'.split()\n",
    "        primary_header_shape = (header['SUBSIZE2'], header['SUBSIZE1'])\n",
    "        for ext in extensions:\n",
    "            try:\n",
    "                header = fits.getheader(filename, ext)\n",
    "                data_shape = fits.getdata(filename, ext).shape\n",
    "                naxis_shape = (header['NAXIS2'], header['NAXIS1'])\n",
    "                if ((primary_header_shape != data_shape) | (primary_header_shape != naxis_shape) | \n",
    "                    (primary_header_shape != expected_shape)):\n",
    "                    print('WARNING: Data dimensions incorrect')\n",
    "                    print('Expected image shape: {}'.format(expected_shape))\n",
    "                    print('PRIMARY header shape: {}'.format(primary_header_shape))\n",
    "                    print('{} header shape: {}'.format(ext, naxis_shape))\n",
    "                    print('{} data shape: {}'.format(ext, data_shape))\n",
    "            except KeyError:\n",
    "                print('Cannot verify shape for {} extension'.format(ext))\n",
    "    else:\n",
    "        print('File type {} not supported for dimension checks'.format(file_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_extensions(filename, file_type):\n",
    "    \"\"\"Verify that the expected extensions exist\n",
    "    \"\"\"\n",
    "    if file_type == 'UNCAL':\n",
    "        extensions = 'PRIMARY SCI GROUP INT_TIMES ASDF'.split()\n",
    "    elif file_type == 'DARK':\n",
    "        extensions = 'PRIMARY SCI PIXELDQ GROUPDQ ERR GROUP INT_TIMES ASDF'.split()\n",
    "    elif file_type == 'RATE':\n",
    "        extensions = 'PRIMARY SCI ERR DQ VAR_POISSON VAR_RNOISE ASDF'.split()\n",
    "    else:\n",
    "        print('File type {} not supported for ext verification'.format(file_type))\n",
    "        \n",
    "    for ext in extensions:\n",
    "        try:\n",
    "            header = fits.getheader(filename, ext)\n",
    "        except KeyError:\n",
    "            print('WARNING: {} extension does not exist'.format(ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(xml_file, output_dir, gseg_uncal_files):\n",
    "    \"\"\"MAIN FUNCTION\"\"\"\n",
    "    \n",
    "    read_pattern_def_file = os.path.join(pkg_resources.resource_filename('mirage', ''), \n",
    "                                         'config', 'nircam_read_pattern_definitions.list')\n",
    "    \n",
    "    # Make an empty dictionary to store output summary table info\n",
    "    summary_dict = OrderedDict()\n",
    "    cols = SUMMARY_TABLE_PRIMARY + SUMMARY_TABLE_SCI\n",
    "    for col in cols:\n",
    "        summary_dict[col] = []\n",
    "    \n",
    "    # Find the corresponding rate files for each uncal file\n",
    "    gseg_rate_files = find_rate_files(gseg_uncal_files)\n",
    "    \n",
    "    # Create apt-derived dictionary\n",
    "    pointing_file = xml_file.replace('.xml', '.pointing')\n",
    "    catalogs = {'nircam': {'sw': 'nothing.cat', 'lw': 'nothing.cat'}}\n",
    "    observation_list_file = os.path.join(output_dir, 'observation_list.yaml')\n",
    "    apt_xml_dict = get_observation_dict(xml_file, observation_list_file, catalogs,\n",
    "                                        verbose=True)\n",
    "    \n",
    "    # Find observations to validate, either all in proposal or user-specified\n",
    "    observation_list = set(apt_xml_dict['ObservationID'])\n",
    "    if len(observations) != 0:\n",
    "        str_obs_list = observations\n",
    "    else:\n",
    "        int_obs = sorted([int(o) for o in observation_list])\n",
    "        str_obs_list = [str(o).zfill(3) for o in int_obs]\n",
    "    \n",
    "    for observation_to_check in str_obs_list:\n",
    "        print('')\n",
    "        print('')\n",
    "        print('OBSERVATION: {}'.format(observation_to_check))\n",
    "        print('')\n",
    "        \n",
    "        good = np.where(np.array(apt_xml_dict['ObservationID']) == observation_to_check)\n",
    "        module = apt_xml_dict['Module'][good[0][0]].upper()\n",
    "        \n",
    "        try:\n",
    "            total_expected_files, expected_detectors = calculate_total_files(apt_xml_dict, good[0][0])\n",
    "            print('Total number of expected files: {}'.format(total_expected_files))\n",
    "            print('Expected detectors used: {}'.format(expected_detectors))\n",
    "        except IndexError:\n",
    "            print(\"No files found.\")\n",
    "            continue\n",
    "\n",
    "        # The complication here is that the table created by Mirage does not have a filename\n",
    "        # attached to each entry. So we need a way to connect an actual filename\n",
    "        # to each entry\n",
    "        subdir_start = 'jw' + apt_xml_dict['ProposalID'][good[0][0]] + observation_to_check.zfill(3)\n",
    "        matching_uncal_files = sorted([filename for filename in gseg_uncal_files if subdir_start in filename])\n",
    "        matching_rate_files = sorted([filename for filename in gseg_rate_files if subdir_start in filename])\n",
    "        print('Found uncal files:')\n",
    "        for i in range(len(matching_uncal_files)):\n",
    "            print(matching_uncal_files[i])\n",
    "        print('')\n",
    "        print('Found rate files:')\n",
    "        for i in range(len(matching_rate_files)):\n",
    "            print(matching_rate_files[i])\n",
    "        print('')\n",
    "        \n",
    "        # Check for any missing files and that a file exists for each expected detector used\n",
    "        check_detector_exposures(matching_uncal_files, matching_rate_files, expected_detectors)\n",
    "        \n",
    "        # Deal with the case of matching_uncal_files and matching_rate_files having\n",
    "        # different lengths here. In order to loop over them they must have the same length\n",
    "        if len(matching_uncal_files) != len(matching_rate_files):\n",
    "            (matching_uncal_files, matching_rate_files) = equalize_file_lists(matching_uncal_files, matching_rate_files)\n",
    "            print('Equalized file lists (should have a 1:1 correspondence):')\n",
    "            for idx in range(len(matching_uncal_files)):\n",
    "                print(matching_uncal_files[idx], matching_rate_files[idx])\n",
    "\n",
    "        # Create siaf instance for later calculations\n",
    "        siaf = pysiaf.Siaf('NIRCam')\n",
    "\n",
    "        for file_pair in zip(matching_uncal_files, matching_rate_files):\n",
    "            for f in file_pair: \n",
    "                # Only validate files that exist\n",
    "                good_file = f != None\n",
    "                if good_file:\n",
    "                    if not os.path.isfile(f):\n",
    "                        print('WARNING: File does not exist: {}'.format(f))\n",
    "                        good_file = False\n",
    "                \n",
    "                if good_file:\n",
    "                    print(\"Checking {}\".format(os.path.split(f)[1]))\n",
    "                    print('----------------------------------------------------')\n",
    "                    file_type = f.split('.fits')[0].split('_')[-1].upper()\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Verify that all expected extensions exist for this file and add\n",
    "                # file info to the output summary table\n",
    "                verify_extensions(f, file_type)\n",
    "                summary_dict = add_entry(f, summary_dict)\n",
    "                \n",
    "                # Get info from header to be compared\n",
    "                data, header, sci_header = get_data(f)\n",
    "                header_vals = header_keywords(header)\n",
    "\n",
    "                # Get matching data from the APT exposure table\n",
    "                table_vals = table_info(apt_xml_dict, good[0][0])\n",
    "                \n",
    "                # Verify that the header and data dimensions are correct in each extension\n",
    "                expected_shape = get_expected_shape(table_vals['SUBARRAY'])\n",
    "                verify_dimensions(f, file_type, expected_shape)\n",
    "                \n",
    "                # Check detector/aperture\n",
    "                detector_from_filename = f.split('_')[-2].upper()\n",
    "                header_detector = header['DETECTOR']\n",
    "                aperture = header['APERNAME']  # could also try APERNAME, PPS_APER\n",
    "                if 'LONG' in header_detector:\n",
    "                    header_detector = header_detector.replace('LONG', '5')\n",
    "                if header_detector not in aperture:\n",
    "                    print((\"WARNING: Detector name and aperture name in file header appear to be incompatible: {}, {}\"\n",
    "                          .format(header['DETECTOR'], aperture)))\n",
    "                    print(\"Detector listed in filename: {}\".format(detector_from_filename))\n",
    "                    print('If the aperture is incorrect then the calculated subarray '\n",
    "                          'location from pysiaf will also be incorrect.')\n",
    "                check_apername(table_vals['SUBARRAY'], aperture)  # make sure APT subarray is consistent with header APERNAME\n",
    "                data_shape = data.shape\n",
    "                \n",
    "                # Compare NFRAME, GROUPGAP from header with expected values based on READPATT\n",
    "                m = Catalog_seed()\n",
    "                params = {'Readout': {'readpatt': table_vals['READPATT']},\n",
    "                          'Reffiles': {'readpattdefs': read_pattern_def_file}}\n",
    "                m.params = params\n",
    "                m.read_pattern_check()\n",
    "                nframes = m.params['Readout']['nframe']\n",
    "                groupgap = m.params['Readout']['nskip']\n",
    "                if nframes != header['NFRAMES']:\n",
    "                    print('WARNING: NFRAME mismatch between header ({}) and expected value ({}).'.format(\n",
    "                          nframes, header['NFRAMES']))\n",
    "                if groupgap != header['GROUPGAP']:\n",
    "                    print('WARNING: GROUPGAP mismatch between header ({}) and expected value ({}).'.format(\n",
    "                          groupgap, header['GROUPGAP']))\n",
    "\n",
    "                # Make some adjustments to the exposure table info\n",
    "\n",
    "                # Calucate the exposure time\n",
    "                if 'FULL' in table_vals['SUBARRAY']:\n",
    "                    num_amps = 4\n",
    "                else:  # assume all grism and sub use 1 amp outputs\n",
    "                    num_amps = 1\n",
    "                frametime = calc_frame_time('NIRCam', aperture, data_shape[-1], data_shape[-2], num_amps)\n",
    "                table_vals['EFFEXPTM'] = frametime * (table_vals['NGROUPS'] * (nframes+groupgap) - groupgap)\n",
    "\n",
    "                # NAXIS\n",
    "                table_vals['NAXIS'] = len(data.shape)\n",
    "                header_vals['NAXIS'] = sci_header['NAXIS']\n",
    "\n",
    "                # Use pysiaf to calculate subarray locations\n",
    "                try:\n",
    "                    xc, yc = sci_subarray_corners('NIRCam', aperture, siaf=siaf)\n",
    "                    table_vals['SUBSTRT1'] = xc[0] + 1\n",
    "                    table_vals['SUBSTRT2'] = yc[0] + 1\n",
    "                    table_vals['SUBSIZE1'] = siaf[aperture].XSciSize\n",
    "                    table_vals['SUBSIZE2'] = siaf[aperture].YSciSize\n",
    "                except KeyError:\n",
    "                    print(\"ERROR: Aperture {} is not a valid aperture in pysiaf.\".format(aperture))\n",
    "                    xc = [-2, -2]\n",
    "                    yc = [-2, -2]\n",
    "                    table_vals['SUBSTRT1'] = xc[0] + 1\n",
    "                    table_vals['SUBSTRT2'] = yc[0] + 1\n",
    "                    table_vals['SUBSIZE1'] = 9999\n",
    "                    table_vals['SUBSIZE2'] = 9999\n",
    "\n",
    "                # Create FASTAXIS and SLOWAXIS values based on the detector name\n",
    "                fast, slow = find_fastaxis(header_vals['DETECTOR'])\n",
    "                table_vals['FASTAXIS'] = fast\n",
    "                table_vals['SLOWAXIS'] = slow\n",
    "\n",
    "                # Remove whitespace from observing template in file\n",
    "                header_vals['TEMPLATE'] = header_vals['TEMPLATE'].replace(' ', '').lower()\n",
    "                table_vals['TEMPLATE'] = table_vals['TEMPLATE'].lower()\n",
    "\n",
    "                # Adjust prime/parallel boolean from table to be a string\n",
    "                if not table_vals['EXPRIPAR']:\n",
    "                    table_vals['EXPRIPAR'] = 'PRIME'\n",
    "                else:\n",
    "                    table_vals['EXPRIPAR'] = 'PARALLEL'\n",
    "\n",
    "                # Change exposure type from table to match up with\n",
    "                # types of strings in the file\n",
    "                table_vals['EXP_TYPE'] = adjust_exptype(table_vals['EXP_TYPE'])\n",
    "\n",
    "                # Set the DETECTOR field to be identical. This info is not in the\n",
    "                # exposure table, so we can't actually check it\n",
    "                table_vals['DETECTOR'] = header_vals['DETECTOR']\n",
    "\n",
    "                # Now compare the data in the dictionary from the file versus that\n",
    "                # from the exposure table created from the APT file\n",
    "                err = False\n",
    "                for key in header_vals:\n",
    "                    if header_vals[key] != table_vals[key]:\n",
    "                        if key not in FLOAT_KEYWORDS and key not in FILTER_KEYWORDS:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, in exp table: {}, in file: {}'.format(key, table_vals[key], header_vals[key]))\n",
    "                        elif key in FLOAT_KEYWORDS:\n",
    "                            if not np.isclose(header_vals[key], table_vals[key], rtol=0.01, atol=0.):\n",
    "                                err = True\n",
    "                                print('MISMATCH: {}, in exp table: {}, in file: {}'.format(key, table_vals[key], header_vals[key]))\n",
    "\n",
    "                        if key in ['LONGFILTER', 'LONGPUPIL'] and 'LONG' in header_vals['DETECTOR']:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, in exp table: {}, in file: {}'.format(key, table_vals[key], header_vals[key]))\n",
    "                        if key in ['SHORTFILTER', 'SHORTPUPIL'] and 'LONG' not in header_vals['DETECTOR']:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, in exp table: {}, in file: {}'.format(key, table_vals[key], header_vals[key]))\n",
    "                \n",
    "                # Perform direct comparison between header keywords and their expected values\n",
    "                if file_type == 'UNCAL':\n",
    "                    for key in UNCAL_PRIMARY_KEYWORDS:\n",
    "                        if UNCAL_PRIMARY_KEYWORDS[key] != header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, UNCAL_PRIMARY_KEYWORDS[key], header[key]))\n",
    "                    for key in UNCAL_SCI_KEYWORDS:\n",
    "                        if UNCAL_SCI_KEYWORDS[key] != sci_header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, UNCAL_SCI_KEYWORDS[key], sci_header[key]))\n",
    "                elif file_type == 'DARK':\n",
    "                    for key in DARK_PRIMARY_KEYWORDS:\n",
    "                        if DARK_PRIMARY_KEYWORDS[key] != header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, DARK_PRIMARY_KEYWORDS[key], header[key]))\n",
    "                    for key in DARK_SCI_KEYWORDS:\n",
    "                        if DARK_SCI_KEYWORDS[key] != sci_header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, DARK_SCI_KEYWORDS[key], sci_header[key]))\n",
    "                elif file_type == 'RATE':\n",
    "                    for key in RATE_PRIMARY_KEYWORDS:\n",
    "                        if RATE_PRIMARY_KEYWORDS[key] != header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, RATE_PRIMARY_KEYWORDS[key], header[key]))\n",
    "                    for key in RATE_SCI_KEYWORDS:\n",
    "                        if RATE_SCI_KEYWORDS[key] != sci_header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, RATE_SCI_KEYWORDS[key], sci_header[key]))\n",
    "                else:\n",
    "                    print('No direct header checks performed for {} file type.'.format(file_type))\n",
    "\n",
    "                if not err:\n",
    "                    print('No inconsistencies. File header info correct.')\n",
    "                    \n",
    "                print('')\n",
    "\n",
    "            print('')\n",
    "            print('')\n",
    "    \n",
    "    # Output the summary table\n",
    "    summary_table = Table(summary_dict)\n",
    "    summary_table.write(os.path.join(output_dir, 'summary_table.txt'), format='ascii.fixed_width_two_line')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = '/path/to/proposal/xml/file/00617.xml'\n",
    "output_dir = '/location/to/place/outputs/'\n",
    "gseg_uncal_files = glob('/path/to/gseg/files/*uncal.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(xml_file, output_dir, gseg_uncal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
